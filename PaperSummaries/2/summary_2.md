This paper presents several extensions of Skip-gram model. The Skip-gram model is able to efficiently learns high-quality distributed vector representations capturing word relationships. However, it is limited by large training cost and inability to represent idiomatic phrases. The authors make contributions include (1) subsampling of frequent words and negative sampling (as alternative to hierarchical softmax) for speedup, (2) extension from word-based model to _phrase_-based model, and (3) discovering of vector addition producing meaningful results.

The Skip-gram model objects to maximize the average log (conditional) probability that a word appears as a neighbor of another word (a softmax function is used to formalize this probability). The cost of computation for probability gradient is proportional to the size of vocabulary W, so more efficient alternatives are needed. Hierarchical Softmax, as one choice,  uses a binary tree representation of the output layer with each node representing the relative probabilities of its child nodes.  By reaching a word leaf node from the tree root, the length of the appropriate path is not greater than Log2(W), which implies that the computational cost is optimized from O(W) to O(log2(W)). In addition, Negative Sampling (similar to Noise Contrastive Estimation) aiming to distinguish the target word from noise distribution by logistic regression is another simple but efficient option.

Moreover, subsampling of frequent words helps boost the training speed (and improve accuracy at least for some cases). Note that most frequent words usually provide less information than those rare words. A heuristic formula is used to calculate probability from frequency by &quot;aggressively&quot; subsampling words with frequencies exceeding set threshold while preserving the rankings.

From experimental results of analogous reasoning tests, Negative Sampling performs better than Hierarchical Softmax and even Noise Contrastive Estimation. And the _linearity_ of skip-gram model is beneficial for vectors in linear analogous reasoning tasks.

To learn (vector representation for) phrases, (1) a data-driven approach based on a formula about unigram and bigram counts are run, (2) phrases are viewed as _individual_ tokens (just like words?) during model training, (3) tests of analogical reasoning containing both words and phrases are designed and utilized. With huge amount of input data and careful hyper-parameter tuning, the best trained model results in accuracy of 72%.

Interestingly, addition of vector &quot;Russia&quot; and &quot;river&quot; returns &quot;Volga River&quot;. The training objective might give an explanation: word vectors are trained to predict surrounding words in same sentences, which might also be seen as distribution of context where the word appears. Note that the objective takes logarithm, converting vector addition to distribution product, which is analogous to AND function validating those words being assigned high probabilities by both multiplied word vectors. Finally, the authors mention that combination of (1) &quot;compositional&quot; vector addition and (2) representing phrases with a _single_ token might give simple and powerful approach to representation of longer pieces of text.